{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUF4bgQB517HhTJPKiUzs1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodriguessdeyson/feature-comparison/blob/master-artefacts/feature-selection/FeatureSelection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seleção de features dados estatísticos\n",
        "Nesta etapa, dois arquivos são carregados com features obtidas dos dados puros de vibração. Esta etapa realiza a compração de dois conjuntos de features comumente extraídas de dados de vibração em comparação com as feature da biblioteca pyAudio (avaliar outra).\n",
        "\n",
        "Após extraídas, um processo de seleção de features é aplicado aos dois conjuntos, sendo eles: Pearson, Gini Gain, Information Gain e Lasso. Ao final, as features selecionadas serão submetidas a dois classificadores e avaliado suas caracterísitcas."
      ],
      "metadata": {
        "id": "76kqM5dJs7Ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga de dados dos dataset com as rotulações de cada falha."
      ],
      "metadata": {
        "id": "XqKos_RZ5wh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mutual_info_score, normalized_mutual_info_score\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Incializa o serviço do Google Drive.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Soq80l5Z6KqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carrega o data e realiza a separação de features e classes"
      ],
      "metadata": {
        "id": "Ygji6d8d7fqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKr1wyXcsVak"
      },
      "outputs": [],
      "source": [
        "# Load your CSV file into a DataFrame\n",
        "data_statistics = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Master Final Project/V2/statistics_features.csv')\n",
        "data_statistics = shuffle(data_statistics)\n",
        "\n",
        "\n",
        "# Separate the features and the target variable\n",
        "X = data_statistics.drop('class', axis=1)  # Features\n",
        "y = data_statistics['class']  # Target variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_statistics"
      ],
      "metadata": {
        "id": "ecASuDlgsLQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection com Pearson’s Correlation Coefficient\n"
      ],
      "metadata": {
        "id": "2c3VSkTx55tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando a seleção de features com pearson, conseguimos remover features que são, de certa forma, semelhantes, mantendo apenas uma delas, evitano do sobrecarga do classificador. Para este modo de seleção, extrairemos as 10 principais features e testaresmos no classificador."
      ],
      "metadata": {
        "id": "KhbdNianNs7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, SelectFromModel\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Calculate Pearson correlation coefficients\n",
        "pearson_correlation = X.corrwith(y)\n",
        "\n",
        "# Select top features based on absolute correlation values\n",
        "top_features = pearson_correlation.abs().sort_values(ascending=False).head(15).index\n",
        "\n",
        "# Feature selection using Pearson correlation\n",
        "pearson_features = SelectKBest(score_func=f_classif, k=15)\n",
        "pearson_features.fit(X[top_features], y)\n",
        "\n",
        "# Get selected features and their scores\n",
        "pearson_selected_features = X[top_features].columns[pearson_features.get_support(indices=True)]\n",
        "pearson_feature_scores = pearson_features.scores_[pearson_features.get_support(indices=True)]\n",
        "\n",
        "print(pearson_selected_features)"
      ],
      "metadata": {
        "id": "_E4I-yK_5_Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Assuming X and y are your feature matrix and target variable\n",
        "\n",
        "# Calculate Pearson correlation coefficients\n",
        "pearson_correlation = X.apply(lambda x: pearsonr(x, y)[0])\n",
        "\n",
        "# Select top features based on absolute correlation values\n",
        "top_features = pearson_correlation.abs().sort_values(ascending=False).head(15).index\n",
        "\n",
        "# Use SelectKBest with f_classif to select the top features\n",
        "pearson_features = SelectKBest(score_func=f_classif, k=15)\n",
        "pearson_features.fit(X[top_features], y)\n",
        "\n",
        "# Get selected features and their scores\n",
        "pearson_selected_features = X[top_features].columns[pearson_features.get_support(indices=True)]\n",
        "pearson_feature_scores = pearson_features.scores_[pearson_features.get_support(indices=True)]\n",
        "\n",
        "# Print the selected features and their scores\n",
        "print(\"Selected features using Pearson correlation:\", pearson_selected_features)\n",
        "print(\"Feature scores:\", pearson_feature_scores)"
      ],
      "metadata": {
        "id": "Fcya8Ig31Thr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection com Information Gain Ratio"
      ],
      "metadata": {
        "id": "WrzNoVtH6cP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Feature extraction using Information Gain\n",
        "information_gain_features = SelectKBest(score_func=mutual_info_classif, k=15)  # Select top 10 features\n",
        "information_gain_features.fit(X, y)\n",
        "information_gain_selected_features = X.columns[information_gain_features.get_support(indices=True)]\n",
        "\n",
        "print(\"Selected features using Information Gain:\", information_gain_selected_features)\n"
      ],
      "metadata": {
        "id": "jxrZX_-H6f1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection com Gini Gain"
      ],
      "metadata": {
        "id": "glCbSysW7Kr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest.fit(X, y)\n",
        "gini_selected_features = X.columns[forest.feature_importances_.argsort()[::-1][:15]]  # Select top 10 features\n",
        "\n",
        "print(\"Selected features using Gini gain:\", gini_selected_features)"
      ],
      "metadata": {
        "id": "sOIhI_Ia7LED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection com LassoCV"
      ],
      "metadata": {
        "id": "BDL4SMUu7Zw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection using LassoCV\n",
        "lasso_cv = LassoCV(cv=15)  # You can adjust the number of cross-validation folds\n",
        "lasso_cv.fit(X, y)\n",
        "lasso_selected_features = X.columns[SelectFromModel(lasso_cv, prefit=True).get_support(indices=True)]\n",
        "print(\"Selected features using LASSO:\", lasso_selected_features)"
      ],
      "metadata": {
        "id": "zuVD10a07dKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank das features selecionadas"
      ],
      "metadata": {
        "id": "icPvC2xR8ian"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, SelectFromModel\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X and y are already defined\n",
        "\n",
        "# Feature selection using Pearson correlation\n",
        "pearson_features = SelectKBest(score_func=f_classif, k=15)  # Select top 15 features\n",
        "pearson_features.fit(X, y)\n",
        "pearson_selected_features = X.columns[pearson_features.get_support(indices=True)]\n",
        "pearson_feature_scores = pearson_features.scores_[pearson_features.get_support(indices=True)]\n",
        "\n",
        "# Feature selection using Information Gain\n",
        "information_gain_features = SelectKBest(score_func=mutual_info_classif, k=15)  # Select top 15 features\n",
        "information_gain_features.fit(X, y)\n",
        "information_gain_selected_features = X.columns[information_gain_features.get_support(indices=True)]\n",
        "information_gain_scores = information_gain_features.scores_[information_gain_features.get_support(indices=True)]\n",
        "\n",
        "# Feature selection using Gini gain (Random Forest)\n",
        "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest.fit(X, y)\n",
        "gini_selected_features = X.columns[forest.feature_importances_.argsort()[::-1][:15]]  # Select top 15 features\n",
        "gini_importance_scores = forest.feature_importances_[forest.feature_importances_.argsort()[::-1][:15]]\n",
        "\n",
        "# Feature selection using LassoCV\n",
        "lasso_cv = LassoCV(cv=15)  # You can adjust the number of cross-validation folds\n",
        "lasso_cv.fit(X, y)\n",
        "lasso_selected_features = X.columns[SelectFromModel(lasso_cv, prefit=True, max_features=15).get_support(indices=True)]\n",
        "lasso_coefficients = lasso_cv.coef_[SelectFromModel(lasso_cv, prefit=True, max_features=15).get_support(indices=True)]\n",
        "\n",
        "# Create separate bar plots for each method\n",
        "methods = ['Pearson', 'Information Gain', 'Gini Importance', 'LassoCV Coefficient']\n",
        "all_selected_features = [pearson_selected_features, information_gain_selected_features, gini_selected_features, lasso_selected_features]\n",
        "all_scores = [pearson_feature_scores, information_gain_scores, gini_importance_scores, lasso_coefficients]\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "for i, method in enumerate(methods):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.bar(range(len(all_selected_features[i])), all_scores[i][all_selected_features[i].get_indexer(all_selected_features[i])], color='skyblue')\n",
        "    plt.title(f'{method} Feature Ranking')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.xticks(range(len(all_selected_features[i])), all_selected_features[i], rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FWv9syLuSGHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, SelectFromModel\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X and y are already defined\n",
        "\n",
        "# Calculate Pearson correlation coefficients\n",
        "pearson_correlation = X.apply(lambda x: pearsonr(x, y)[0])\n",
        "\n",
        "# Select top features based on absolute correlation values\n",
        "top_features = pearson_correlation.abs().sort_values(ascending=False).head(15).index\n",
        "\n",
        "# Feature selection using Pearson correlation\n",
        "pearson_features = SelectKBest(score_func=f_classif, k=15)\n",
        "pearson_features.fit(X[top_features], y)\n",
        "\n",
        "# Get selected features and their scores\n",
        "pearson_selected_features = X[top_features].columns[pearson_features.get_support(indices=True)]\n",
        "pearson_feature_scores = pearson_features.scores_[pearson_features.get_support(indices=True)]\n",
        "\n",
        "# Feature selection using Information Gain\n",
        "information_gain_features = SelectKBest(score_func=mutual_info_classif, k=15)  # Select top 15 features\n",
        "information_gain_features.fit(X, y)\n",
        "information_gain_selected_features = X.columns[information_gain_features.get_support(indices=True)]\n",
        "information_gain_scores = information_gain_features.scores_[information_gain_features.get_support(indices=True)]\n",
        "\n",
        "# Feature selection using Gini gain (Random Forest)\n",
        "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest.fit(X, y)\n",
        "gini_selected_features = X.columns[forest.feature_importances_.argsort()[::-1][:15]]  # Select top 15 features\n",
        "gini_importance_scores = forest.feature_importances_[forest.feature_importances_.argsort()[::-1][:15]]\n",
        "\n",
        "# Feature selection using LassoCV\n",
        "lasso_cv = LassoCV(cv=15)  # You can adjust the number of cross-validation folds\n",
        "lasso_cv.fit(X, y)\n",
        "lasso_selected_features = X.columns[SelectFromModel(lasso_cv, prefit=True, max_features=15).get_support(indices=True)]\n",
        "lasso_coefficients = lasso_cv.coef_[SelectFromModel(lasso_cv, prefit=True, max_features=15).get_support(indices=True)]\n",
        "\n",
        "# Create separate bar plots for each method\n",
        "methods = ['Pearson', 'Information Gain', 'Gini Importance', 'LassoCV Coefficient']\n",
        "all_selected_features = [pearson_selected_features, information_gain_selected_features, gini_selected_features, lasso_selected_features]\n",
        "all_scores = [pearson_feature_scores, information_gain_scores, gini_importance_scores, lasso_coefficients]\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "for i, method in enumerate(methods):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.bar(range(len(all_selected_features[i])), all_scores[i][all_selected_features[i].get_indexer(all_selected_features[i])], color='skyblue')\n",
        "    plt.title(f'{method} Feature Ranking')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.xticks(range(len(all_selected_features[i])), all_selected_features[i], rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NEHKkRGy3NSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, SelectFromModel\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X and y are already defined\n",
        "\n",
        "# Calculate Pearson correlation coefficients\n",
        "pearson_correlation = X.corrwith(y)\n",
        "\n",
        "# Select top features based on absolute correlation values\n",
        "top_features = pearson_correlation.abs().sort_values(ascending=False).head(15).index\n",
        "\n",
        "# Feature selection using Pearson correlation\n",
        "pearson_features = SelectKBest(score_func=f_classif, k=15)\n",
        "pearson_features.fit(X[top_features], y)\n",
        "\n",
        "# Get selected features and their scores\n",
        "pearson_selected_features = X[top_features].columns[pearson_features.get_support(indices=True)]\n",
        "pearson_feature_scores = pearson_features.scores_[pearson_features.get_support(indices=True)]\n",
        "\n",
        "# Feature selection using Information Gain\n",
        "information_gain_features = SelectKBest(score_func=mutual_info_classif, k=15)  # Select top 15 features\n",
        "information_gain_features.fit(X, y)\n",
        "information_gain_selected_features = X.columns[information_gain_features.get_support(indices=True)]\n",
        "information_gain_scores = information_gain_features.scores_[information_gain_features.get_support(indices=True)]\n",
        "\n",
        "# Feature selection using Gini gain (Random Forest)\n",
        "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest.fit(X, y)\n",
        "gini_selected_features = X.columns[forest.feature_importances_.argsort()[::-1][:15]]  # Select top 15 features\n",
        "gini_importance_scores = forest.feature_importances_[forest.feature_importances_.argsort()[::-1][:15]]\n",
        "\n",
        "# Feature selection using LassoCV\n",
        "lasso_cv = LassoCV(cv=10)  # You can adjust the number of cross-validation folds\n",
        "lasso_cv.fit(X, y)\n",
        "lasso_selected_features = X.columns[SelectFromModel(lasso_cv, prefit=True, max_features=15).get_support(indices=True)]\n",
        "lasso_coefficients = lasso_cv.coef_[SelectFromModel(lasso_cv, prefit=True, max_features=15).get_support(indices=True)]\n",
        "\n",
        "# Create separate bar plots for each method\n",
        "methods = ['Pearson', 'Information Gain', 'Gini Importance', 'LassoCV Coefficient']\n",
        "all_selected_features = [pearson_selected_features, information_gain_selected_features, gini_selected_features, lasso_selected_features]\n",
        "all_scores = [pearson_feature_scores, information_gain_scores, gini_importance_scores, lasso_coefficients]\n",
        "\n",
        "plt.figure(figsize=(10, 16))\n",
        "\n",
        "for i, method in enumerate(methods):\n",
        "    plt.subplot(4, 1, i + 1)\n",
        "    plt.bar(range(len(all_selected_features[i])), all_scores[i][all_selected_features[i].get_indexer(all_selected_features[i])], color='skyblue')\n",
        "    plt.title(f'{method} Feature Ranking')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.xticks(range(len(all_selected_features[i])), all_selected_features[i], rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q1Ek7VExBqrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dicionário para armazenar os ranks\n",
        "feature_ranks = {}\n",
        "\n",
        "# Rank baseado nos índices ou pontos dos métodos de seleção de features.\n",
        "for idx, feature in enumerate(pearson_selected_features):\n",
        "    feature_ranks[feature] = idx + 1\n",
        "\n",
        "for idx, feature in enumerate(information_gain_selected_features):\n",
        "    if feature in feature_ranks:\n",
        "        feature_ranks[feature] += idx + 1\n",
        "    else:\n",
        "        feature_ranks[feature] = idx + 1\n",
        "\n",
        "for idx, feature in enumerate(lasso_selected_features):\n",
        "    if feature in feature_ranks:\n",
        "        feature_ranks[feature] += idx + 1\n",
        "    else:\n",
        "        feature_ranks[feature] = idx + 1\n",
        "\n",
        "for idx, feature in enumerate(gini_selected_features):\n",
        "    if feature in feature_ranks:\n",
        "        feature_ranks[feature] += idx + 1\n",
        "    else:\n",
        "        feature_ranks[feature] = idx + 1\n",
        "\n",
        "# Sort the features based on their ranks\n",
        "sorted_features = sorted(feature_ranks, key=feature_ranks.get)\n",
        "\n",
        "# Print the ranked features\n",
        "print(\"Ranked Features:\")\n",
        "for rank, feature in enumerate(sorted_features, 1):\n",
        "    print(f\"Rank {rank}: {feature}\")"
      ],
      "metadata": {
        "id": "Lu8K2IJd8l0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seleção de features dados pyAudio\n",
        "Nesta etapa, dois arquivos são carregados com features obtidas dos dados puros de vibração. Esta etapa realiza a compração de dois conjuntos de features comumente extraídas de dados de vibração em comparação com as feature da biblioteca pyAudio (avaliar outra).\n",
        "\n",
        "Após extraídas, um processo de seleção de features é aplicado aos dois conjuntos, sendo eles: Pearson, Gini Gain, Information Gain e Lasso. Ao final, as features selecionadas serão submetidas a dois classificadores e avaliado suas caracterísitcas."
      ],
      "metadata": {
        "id": "NxZkUA1FblCi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxB7OWNddPQn"
      },
      "outputs": [],
      "source": [
        "# Load your CSV file into a DataFrame\n",
        "data_pyaudio = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Master Final Project/V2/vib_pyaudio_features.csv')\n",
        "data_pyaudio = shuffle(data_pyaudio)\n",
        "\n",
        "\n",
        "# Separate the features and the target variable\n",
        "XX = data_pyaudio.drop('class', axis=1)  # Features\n",
        "yy = data_pyaudio['class']  # Target variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_pyaudio"
      ],
      "metadata": {
        "id": "3h8IV-huO6NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection com Pearson’s Correlation Coefficient\n"
      ],
      "metadata": {
        "id": "VBTPK-TIcPDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, SelectFromModel\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Calculate Pearson correlation coefficients\n",
        "pearson_correlation = XX.corrwith(yy)\n",
        "\n",
        "# Select top features based on absolute correlation values\n",
        "top_features = pearson_correlation.abs().sort_values(ascending=False).head(15).index\n",
        "\n",
        "# Feature selection using Pearson correlation\n",
        "pearson_features = SelectKBest(score_func=f_classif, k=15)\n",
        "pearson_features.fit(XX[top_features], yy)\n",
        "\n",
        "# Get selected features and their scores\n",
        "pearson_selected_features = XX[top_features].columns[pearson_features.get_support(indices=True)]\n",
        "pearson_feature_scores = pearson_features.scores_[pearson_features.get_support(indices=True)]\n",
        "\n",
        "print(pearson_selected_features)"
      ],
      "metadata": {
        "id": "O99SPqvmcPDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection com Information Gain Ratio"
      ],
      "metadata": {
        "id": "ndYbjsXAcXkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Feature extraction using Information Gain\n",
        "information_gain_features = SelectKBest(score_func=mutual_info_classif, k=15)  # Select top 10 features\n",
        "information_gain_features.fit(XX, yy)\n",
        "information_gain_selected_features = XX.columns[information_gain_features.get_support(indices=True)]\n",
        "\n",
        "print(\"Selected features using Information Gain:\", information_gain_selected_features)\n"
      ],
      "metadata": {
        "id": "TeEkvC84cXkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection com Gini Gain"
      ],
      "metadata": {
        "id": "F9-UKsdLccCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest.fit(XX, yy)\n",
        "gini_selected_features = XX.columns[forest.feature_importances_.argsort()[::-1][:15]]  # Select top 5 features\n",
        "\n",
        "print(\"Selected features using Gini gain:\", gini_selected_features)"
      ],
      "metadata": {
        "id": "oAH2_ZIVccCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection com LassoCV"
      ],
      "metadata": {
        "id": "xN2soDCIcjtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection using LassoCV\n",
        "lasso_cv = LassoCV(cv=10)  # You can adjust the number of cross-validation folds\n",
        "lasso_cv.fit(XX, yy)\n",
        "lasso_selected_features = XX.columns[SelectFromModel(lasso_cv, prefit=True, max_features=15).get_support(indices=True)]\n",
        "print(\"Selected features using LASSO:\", lasso_selected_features[:15])"
      ],
      "metadata": {
        "id": "mNp63idHcjtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank das features selecionadas"
      ],
      "metadata": {
        "id": "TRdkz6ilcobC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dicionário para armazenar os ranks\n",
        "feature_ranks_pyAudio = {}\n",
        "\n",
        "# Rank baseado nos índices ou pontos dos métodos de seleção de features.\n",
        "for idx, feature in enumerate(pearson_selected_features):\n",
        "    feature_ranks_pyAudio[feature] = idx + 1\n",
        "\n",
        "for idx, feature in enumerate(information_gain_selected_features):\n",
        "    if feature in feature_ranks_pyAudio:\n",
        "        feature_ranks_pyAudio[feature] += idx + 1\n",
        "    else:\n",
        "        feature_ranks_pyAudio[feature] = idx + 1\n",
        "\n",
        "for idx, feature in enumerate(lasso_selected_features):\n",
        "    if feature in feature_ranks_pyAudio:\n",
        "        feature_ranks_pyAudio[feature] += idx + 1\n",
        "    else:\n",
        "        feature_ranks_pyAudio[feature] = idx + 1\n",
        "\n",
        "for idx, feature in enumerate(gini_selected_features):\n",
        "    if feature in feature_ranks_pyAudio:\n",
        "        feature_ranks_pyAudio[feature] += idx + 1\n",
        "    else:\n",
        "        feature_ranks_pyAudio[feature] = idx + 1\n",
        "\n",
        "# Sort the features based on their ranks\n",
        "sorted_features = sorted(feature_ranks_pyAudio, key=feature_ranks_pyAudio.get)\n",
        "\n",
        "# Print the ranked features\n",
        "print(\"Ranked Features:\")\n",
        "for rank, feature in enumerate(sorted_features, 1):\n",
        "    print(f\"Rank {rank}: {feature}\")"
      ],
      "metadata": {
        "id": "fvuGKAG3cobC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, SelectFromModel\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X and y are already defined\n",
        "\n",
        "# Calculate Pearson correlation coefficients\n",
        "pearson_correlation = XX.corrwith(yy)\n",
        "\n",
        "# Select top features based on absolute correlation values\n",
        "top_features = pearson_correlation.abs().sort_values(ascending=False).head(15).index\n",
        "\n",
        "# Feature selection using Pearson correlation\n",
        "pearson_features = SelectKBest(score_func=f_classif, k=15)\n",
        "pearson_features.fit(XX[top_features], yy)\n",
        "\n",
        "# Get selected features and their scores\n",
        "pearson_selected_features = XX[top_features].columns[pearson_features.get_support(indices=True)]\n",
        "pearson_feature_scores = pearson_features.scores_[pearson_features.get_support(indices=True)]\n",
        "\n",
        "# Feature selection using Information Gain\n",
        "information_gain_features = SelectKBest(score_func=mutual_info_classif, k=15)  # Select top 15 features\n",
        "information_gain_features.fit(XX, yy)\n",
        "information_gain_selected_features = XX.columns[information_gain_features.get_support(indices=True)]\n",
        "information_gain_scores = information_gain_features.scores_[information_gain_features.get_support(indices=True)]\n",
        "\n",
        "# Feature selection using Gini gain (Random Forest)\n",
        "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest.fit(XX, yy)\n",
        "gini_selected_features = XX.columns[forest.feature_importances_.argsort()[::-1][:15]]  # Select top 15 features\n",
        "gini_importance_scores = forest.feature_importances_[forest.feature_importances_.argsort()[::-1][:15]]\n",
        "\n",
        "# Feature selection using LassoCV\n",
        "lasso_cv = LassoCV(cv=10)  # You can adjust the number of cross-validation folds\n",
        "lasso_cv.fit(XX, yy)\n",
        "lasso_selected_features = XX.columns[SelectFromModel(lasso_cv, prefit=True, max_features=15).get_support(indices=True)]\n",
        "lasso_coefficients = lasso_cv.coef_[SelectFromModel(lasso_cv, prefit=True, max_features=15).get_support(indices=True)]\n",
        "\n",
        "# Create separate bar plots for each method\n",
        "methods = ['Pearson', 'Information Gain', 'Gini Importance', 'LassoCV Coefficient']\n",
        "all_selected_features = [pearson_selected_features, information_gain_selected_features, gini_selected_features, lasso_selected_features]\n",
        "all_scores = [pearson_feature_scores, information_gain_scores, gini_importance_scores, lasso_coefficients]\n",
        "\n",
        "plt.figure(figsize=(10, 16))\n",
        "\n",
        "for i, method in enumerate(methods):\n",
        "    plt.subplot(4, 1, i + 1)\n",
        "    plt.bar(range(len(all_selected_features[i])), all_scores[i][all_selected_features[i].get_indexer(all_selected_features[i])], color='skyblue')\n",
        "    plt.title(f'{method} Feature Ranking')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.xticks(range(len(all_selected_features[i])), all_selected_features[i], rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uENJkWGmnKcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}